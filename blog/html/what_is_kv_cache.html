<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <title>What is KV Cache? A Clear, Step-by-Step Explanation – Rui Li</title>
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <link rel="icon" href="../../assets/img/favicon.png" type="image/png">
  
  <link rel="stylesheet" href="../../assets/css/style.css" />
  <link rel="stylesheet"
        href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.1/css/all.min.css" />

  <!-- Highlight.js (for code blocks) -->
  <link rel="stylesheet"
        href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github.min.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
  <script>
    document.addEventListener("DOMContentLoaded", () => {
      if (window.hljs) {
        window.hljs.highlightAll();
      }
    });
  </script>

  <!-- MathJax (for LaTeX equations) -->
  <script>
    window.MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$', '$$'], ['\\[', '\\]']]
      },
      options: {
        skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
    };
  </script>
  <script id="MathJax-script" async
          src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>

  <header class="site-header">
    <div class="site-header-inner">
      <div class="site-title">Rui Li</div>

      <div class="site-nav">
        <div class="site-nav-links">
          <a href="../../index.html#about">About</a>
          <a href="../index.html" style="margin-left:1.2rem;">Notes / Blog</a>
        </div>
        <div class="header-icons">
          <a class="icon-button"
             href="mailto:rui.li@aalto.fi"
             aria-label="Email">
            <i class="fa-solid fa-envelope"></i>
          </a>
          <a class="icon-button"
             href="https://scholar.google.com/citations?user=2HW6eeUAAAAJ"
             target="_blank"
             aria-label="Google Scholar">
            <i class="fa-solid fa-graduation-cap"></i>
          </a>
          <a class="icon-button"
             href="https://github.com/ruili-pml"
             target="_blank"
             aria-label="GitHub">
            <i class="fa-brands fa-github"></i>
          </a>
          <a class="icon-button"
             href="https://www.linkedin.com/in/ruili-pml/"
             target="_blank"
             aria-label="LinkedIn">
            <i class="fa-brands fa-linkedin"></i>
          </a>
        </div>
      </div>
    </div>
  </header>

  <main class="page-wrapper blog-main">
    <div class="main">
      <div class="blog-layout">
        <!-- main content -->
        <section class="section blog-content">
          <h1 class="blog-post-title">What is KV Cache? A Clear, Step-by-Step Explanation</h1>

          <div class="blog-post-meta">
            2025-12-5
            <span class="blog-post-tags"><span class="tag">KV cache</span><span class="tag">LLM Inference</span></span>
          </div>
          
          
  
          <div class="post-content" id="post-content">
            <p>KV cache (Key–Value cache) is an inference-time optimization used in Large Language Models (LLMs) to speed up generation. The core idea is this: instead of recomputing the <strong>keys</strong> and <strong>values</strong> for all previous tokens at every step, we will cache the computed keys and values and then <strong>reuse</strong> them during decoding.</p>
<p>Two properties of the usual transformer setup make this possible:</p>
<ul>
<li><strong>Causal masking</strong> – token <span class="arithmatex">\(t\)</span> is only allowed to attend to tokens at positions <span class="arithmatex">\(\leq t\)</span>, so once we have the keys and values for earlier tokens, they never change.</li>
<li><strong>Next-token prediction</strong> – the model only needs the representation of the <em>last</em> token to predict the next one, so we don’t need to recompute outputs for the whole sequence each time.</li>
</ul>
<p>KV cache saves a huge amount of computation, especially for long prompts.</p>
<p>To understand why KV cache makes sense, the easiest way, in my opinion, is to write out the computation for a few tokens and see what actually happens.
Once the equations are laid out, it becomes obvious that certain things are being recomputed, and that we can save compute by storing them.</p>
<p>We'll consider a single-layer LLM with <span class="arithmatex">\(H\)</span> attention heads.<br />
For a deeper LLM, the logic is exactly the same, just applied layer by layer.</p>
<p>At inference time there are usually two stages:</p>
<ol>
<li><strong>Prefill</strong>: process the prompt in sequence.</li>
<li><strong>Autoregressive generation</strong>: generate one token at a time and append it to the input.</li>
</ol>
<hr />
<h1 id="two-stage-generation">Two stage generation</h1>
<h2 id="step-1-processing-the-prompt-prefill">Step 1: Processing the Prompt (Prefill)</h2>
<p>For simplicity, let's say the tokenised prompt has two tokens. So the input is
$$
\boldsymbol{X} =
\begin{bmatrix}
\boldsymbol{x}_1 \[1em]
\boldsymbol{x}_2 \
\end{bmatrix}
$$</p>
<p>Here, <span class="arithmatex">\(\boldsymbol{x}_t\)</span> is the embedding (or hidden state) of token <span class="arithmatex">\(t\)</span>.</p>
<h3 id="rmsnorm">RMSNorm</h3>
<p>The first step is normalization. 
In most modern LLMs (like Llama), RMSNorm is applied on a <em>per-token level</em>. This means the normalization of <span class="arithmatex">\(\boldsymbol{x}_1\)</span> does not depend on <span class="arithmatex">\(\boldsymbol{x}_2\)</span>.</p>
<div class="arithmatex">\[
\widetilde{\boldsymbol{X}} =
\begin{bmatrix}
\operatorname{norm}\big(\boldsymbol{x}_1\big) \\[1em]
\operatorname{norm}\big(\boldsymbol{x}_2\big) \\
\end{bmatrix}
=
\begin{bmatrix}
\widetilde{\boldsymbol{x}}_1 \\[1em]
\widetilde{\boldsymbol{x}}_2\\
\end{bmatrix}
\]</div>
<p>So far, there is <em>no interaction between tokens</em>. Each token is just scaled and shifted on its own.
In other words, nothing here will ever depend on the rest of the sequence.</p>
<p>Next we enter the parallel computation of each single-head attention.
This is where the interaction between tokens happens, and it’s also where KV cache will later save us the most work.</p>
<h3 id="single-head-attention-of-head-h">Single head attention of head <span class="arithmatex">\(h\)</span></h3>
<p>For each head <span class="arithmatex">\(h\)</span>, we first compute the query, key and value through linear projections.
$$
\boldsymbol{Q}^{(h)} =
\begin{bmatrix}
\widetilde{\boldsymbol{x}}<em>1 \boldsymbol{W}^{(h)}</em>{Q} \[0.8em]
\widetilde{\boldsymbol{x}}<em>2 \boldsymbol{W}^{(h)}</em>{Q}
\end{bmatrix}
=
\begin{bmatrix}
\boldsymbol{q}^{(h)}<em>1 \[0.8em]
\boldsymbol{q}^{(h)}_2
\end{bmatrix}
,\quad
\boldsymbol{K}^{(h)} =
\begin{bmatrix}
\widetilde{\boldsymbol{x}}_1 \boldsymbol{W}^{(h)}</em>{K} \[0.8em]
\widetilde{\boldsymbol{x}}<em>2 \boldsymbol{W}^{(h)}</em>{K}
\end{bmatrix}
=
\begin{bmatrix}
\boldsymbol{k}^{(h)}<em>1 \[0.8em]
\boldsymbol{k}^{(h)}_2
\end{bmatrix},\quad
\boldsymbol{V}^{(h)} =
\begin{bmatrix}
\widetilde{\boldsymbol{x}}_1 \boldsymbol{W}^{(h)}</em>{V} \[0.8em]
\widetilde{\boldsymbol{x}}<em>2 \boldsymbol{W}^{(h)}</em>{V}
\end{bmatrix}
=
\begin{bmatrix}
\boldsymbol{v}^{(h)}_1 \[0.8em]
\boldsymbol{v}^{(h)}_2
\end{bmatrix}.
$$</p>
<p>Here, the dependence is still local:
- <span class="arithmatex">\(\boldsymbol{q}^{(h)}_t\)</span> depends only on <span class="arithmatex">\(\widetilde{\boldsymbol{x}}_t\)</span> and <span class="arithmatex">\(\boldsymbol{W}^{(h)}_Q\)</span><br />
- <span class="arithmatex">\(\boldsymbol{k}^{(h)}_t\)</span> depends only on <span class="arithmatex">\(\widetilde{\boldsymbol{x}}_t\)</span> and <span class="arithmatex">\(\boldsymbol{W}^{(h)}_K\)</span><br />
- <span class="arithmatex">\(\boldsymbol{v}^{(h)}_t\)</span> depends only on <span class="arithmatex">\(\widetilde{\boldsymbol{x}}_t\)</span> and <span class="arithmatex">\(\boldsymbol{W}^{(h)}_V\)</span></p>
<p>Nothing here mixes information across different tokens yet; we are still just reshaping each token independently into Q/K/V space.</p>
<p>Next, we calculate the attention weights and single-head output.
This is where the mixing begins: each query now “looks at” all previous keys and decides how much to mix from their values.</p>
<p>The masked attention scores are
$$
\boldsymbol{S}^{(h)} =
\begin{bmatrix}
\boldsymbol{q}^{(h)}_1 \cdot \boldsymbol{k}^{(h)}_1 &amp; -\infty \[0.8em]
\boldsymbol{q}^{(h)}_2 \cdot \boldsymbol{k}^{(h)}_1 &amp; \boldsymbol{q}^{(h)}_2 \cdot \boldsymbol{k}^{(h)}_2
\end{bmatrix}.
$$</p>
<p>With a <strong>causal mask</strong>, we ensure that each token cannot see future information. As we will see in a minute, <strong>this also ensures that the hidden states of past tokens will never change when new tokens arrives</strong>.</p>
<p>By applying a row-wise softmax, we get the attention weights.
$$
\boldsymbol{A}^{(h)} =
\begin{bmatrix}
\dfrac{ \exp!\big(\boldsymbol{q}^{(h)}_1\cdot \boldsymbol{k}^{(h)}_1\big) }
      { \exp!\big(\boldsymbol{q}^{(h)}_1\cdot \boldsymbol{k}^{(h)}_1\big) } &amp;
0 \[2.0em]
\dfrac{ \exp!\big(\boldsymbol{q}^{(h)}_2\cdot \boldsymbol{k}^{(h)}_1\big) }
      { \exp!\big(\boldsymbol{q}^{(h)}_2\cdot \boldsymbol{k}^{(h)}_1\big)
       +\exp!\big(\boldsymbol{q}^{(h)}_2\cdot \boldsymbol{k}^{(h)}_2\big) } &amp;
\dfrac{ \exp!\big(\boldsymbol{q}^{(h)}_2\cdot \boldsymbol{k}^{(h)}_2\big) }
      { \exp!\big(\boldsymbol{q}^{(h)}_2\cdot \boldsymbol{k}^{(h)}_1\big)
       +\exp!\big(\boldsymbol{q}^{(h)}_2\cdot \boldsymbol{k}^{(h)}_2\big) }
\end{bmatrix}.
$$</p>
<p>Then, the output of each head is
$$
\begin{bmatrix}
\boldsymbol{o}^{(h)}<em>1 \[0.8em]
\boldsymbol{o}^{(h)}_2
\end{bmatrix}
=
\begin{bmatrix}
A^{(h)}</em>{11} \boldsymbol{v}^{(h)}<em>1 \[0.8em]
A^{(h)}</em>{21} \boldsymbol{v}^{(h)}<em>1 +
A^{(h)}</em>{22} \boldsymbol{v}^{(h)}_2
\end{bmatrix}.
$$</p>
<h3 id="multi-head-attention-output">Multi head attention output</h3>
<p>Multi-head attention simply concatenates the outputs of all heads and mixes them with a final output projection.<br />
There is no information exchange between tokens here.</p>
<div class="arithmatex">\[
\begin{bmatrix}
\boldsymbol{o}_1^{\text{Attn}} \\[1em]
\boldsymbol{o}_2^{\text{Attn}} \\
\end{bmatrix}=
\begin{bmatrix}
\boldsymbol{o}_1^{(1)} &amp; \boldsymbol{o}_1^{(2)} &amp; \ldots &amp;  \boldsymbol{o}_1^{(H)}\\[1em]
\boldsymbol{o}_2^{(1)} &amp; \boldsymbol{o}_2^{(2)} &amp;  \ldots  &amp;\boldsymbol{o}_2^{(H)}\\
\end{bmatrix}
\begin{bmatrix}
\boldsymbol{W}_O^{(1)} \\[1em]
\boldsymbol{W}_O^{(2)} \\[1em]
\vdots \\
\boldsymbol{W}_O^{(H)} \\
\end{bmatrix}=
\begin{bmatrix}
\sum_{h=1}^H \boldsymbol{o}_1^{(h)}\boldsymbol{W}_O^{(h)}  \\[1em]
\sum_{h=1}^H \boldsymbol{o}_2^{(h)}\boldsymbol{W}_O^{(h)}\\
\end{bmatrix}
\]</div>
<h3 id="residual-connection">residual connection</h3>
<p>The residual connection adds back the original token representations:</p>
<div class="arithmatex">\[
\begin{bmatrix}
\boldsymbol{x}^{\text{MLP}}_1 \\[1em]
\boldsymbol{x}^{\text{MLP}}_2\\
\end{bmatrix}=
\begin{bmatrix}
\boldsymbol{o}_1^{\text{Attn}} + \boldsymbol{x}_1\\[1em]
\boldsymbol{o}_2^{\text{Attn}} + \boldsymbol{x}_2 \\
\end{bmatrix}
\]</div>
<h3 id="mlp">MLP</h3>
<p>MLP process each token <strong>independently</strong>, so there is no information exchange between tokens.</p>
<div class="arithmatex">\[
\begin{bmatrix}
\boldsymbol{o}_1^{\text{MLP}} \\[1em]
\boldsymbol{o}_2^{\text{MLP}} \\
\end{bmatrix}=
\begin{bmatrix}
\text{MLP}\Big(\text{norm}(\boldsymbol{x}^{\text{MLP}}_1) \Big) \\[1em]
\text{MLP}\Big(\text{norm}(\boldsymbol{x}^{\text{MLP}}_2) \Big) \\
\end{bmatrix}
\]</div>
<h3 id="read-out-layer-predicting-token-3">Read out layer (predicting token 3)</h3>
<p>Finally, the model predicts the <em>next</em> token, <span class="arithmatex">\(\boldsymbol{x}_3\)</span>, based on the hidden states of the last token (<span class="arithmatex">\(\boldsymbol{x}_2\)</span>):</p>
<div class="arithmatex">\[
\boldsymbol{x}_3 = \text{read out} \left(\text{norm}(\boldsymbol{o}_2^{\text{MLP}} + \boldsymbol{x}^{\text{MLP}}_2) \right)
\]</div>
<h2 id="step-2-generation">Step 2: Generation</h2>
<p>So far, we have only processed the prompt, produced a distribution over token 3, and (conceptually) sampled <span class="arithmatex">\(\boldsymbol{x}_3\)</span> from it.
Now we append <span class="arithmatex">\(\boldsymbol{x}_3\)</span> into the input and continue the generation. This is where the redundancy happens.</p>
<p>Let’s first write out the naive computation, where we simply pretend this is a fresh 3-token input and redo everything from scratch.
I have hightlighted in blue the exact computation that we already performed during the prefill phase.</p>
<p>We feed in all three tokens:
$$
\boldsymbol{X} =
\begin{bmatrix}
\boldsymbol{x}_1 \[1em]
\boldsymbol{x}_2 \[1em]
\boldsymbol{x}_3
\end{bmatrix}
$$</p>
<h3 id="rmsnorm_1">RMSNorm</h3>
<div class="arithmatex">\[
\widetilde{\boldsymbol{X}} =
\begin{bmatrix}
\textcolor{blue}{\operatorname{norm}\big(\boldsymbol{x}_1\big)} \\[1em]
\textcolor{blue}{\operatorname{norm}\big(\boldsymbol{x}_2\big)} \\[1em]
\operatorname{norm}\big(\boldsymbol{x}_3\big)
\end{bmatrix}
=
\begin{bmatrix}
\textcolor{blue}{\widetilde{\boldsymbol{x}}_1} \\[1em]
\textcolor{blue}{\widetilde{\boldsymbol{x}}_2}\\[1em]
\widetilde{\boldsymbol{x}}_3
\end{bmatrix}
\]</div>
<h3 id="single-head-attention-of-head-h_1">Single head attention of head <span class="arithmatex">\(h\)</span></h3>
<p>Compute query, key and values
$$
\boldsymbol{Q}^{(h)} =
\begin{bmatrix}
\textcolor{blue}{\widetilde{\boldsymbol{x}}<em>1 \boldsymbol{W}^{(h)}</em>{Q}} \[1em]
\textcolor{blue}{\widetilde{\boldsymbol{x}}<em>2\boldsymbol{W}^{(h)}</em>{Q}} \[1em]
\widetilde{\boldsymbol{x}}<em>3\boldsymbol{W}^{(h)}</em>{Q}
\end{bmatrix}
=
\begin{bmatrix}
\textcolor{blue}{\boldsymbol{q}^{(h)}<em>1} \[1em]
\textcolor{blue}{\boldsymbol{q}^{(h)}_2} \[1em]
\boldsymbol{q}^{(h)}_3
\end{bmatrix}, \quad
\boldsymbol{K}^{(h)} =
\begin{bmatrix}
\textcolor{blue}{\widetilde{\boldsymbol{x}}_1 \boldsymbol{W}^{(h)}</em>{K}} \[1em]
\textcolor{blue}{\widetilde{\boldsymbol{x}}<em>2\boldsymbol{W}^{(h)}</em>{K}} \[1em]
\widetilde{\boldsymbol{x}}<em>3\boldsymbol{W}^{(h)}</em>{K}
\end{bmatrix}
=
\begin{bmatrix}
\textcolor{blue}{\boldsymbol{k}^{(h)}<em>1} \[1em]
\textcolor{blue}{\boldsymbol{k}^{(h)}_2} \[1em]
\boldsymbol{k}^{(h)}_3
\end{bmatrix}
, \quad
\boldsymbol{V}^{(h)} =
\begin{bmatrix}
\textcolor{blue}{\widetilde{\boldsymbol{x}}_1 \boldsymbol{W}^{(h)}</em>{V}} \[1em]
\textcolor{blue}{\widetilde{\boldsymbol{x}}<em>2 \boldsymbol{W}^{(h)}</em>{V}} \[1em]
\widetilde{\boldsymbol{x}}<em>3 \boldsymbol{W}^{(h)}</em>{V}
\end{bmatrix}
=
\begin{bmatrix}
\textcolor{blue}{\boldsymbol{v}^{(h)}_1} \[1em]
\textcolor{blue}{\boldsymbol{v}^{(h)}_2} \[1em]
\boldsymbol{v}^{(h)}_3
\end{bmatrix}
$$</p>
<p>Attention scores
$$
\boldsymbol{S}^{(h)} =
\begin{bmatrix}
\textcolor{blue}{\boldsymbol{q}^{(h)}_1 \cdot \boldsymbol{k}^{(h)}_1} &amp; -\infty &amp; -\infty \[1em]
\textcolor{blue}{\boldsymbol{q}^{(h)}_2 \cdot \boldsymbol{k}^{(h)}_1} &amp; \textcolor{blue}{\boldsymbol{q}^{(h)}_2 \cdot \boldsymbol{k}^{(h)}_2} &amp; -\infty \[1em]
\boldsymbol{q}^{(h)}_3 \cdot \boldsymbol{k}^{(h)}_1 &amp; \boldsymbol{q}^{(h)}_3 \cdot \boldsymbol{k}^{(h)}_2 &amp; \boldsymbol{q}^{(h)}_3 \cdot \boldsymbol{k}^{(h)}_3
\end{bmatrix}
$$</p>
<p>Attention weights
$$
\boldsymbol{A}^{(h)} =
\begin{bmatrix}
\textcolor{blue}{\dfrac{ \exp!\big(\boldsymbol{q}^{(h)}_1\cdot \boldsymbol{k}^{(h)}_1\big) }
      { \exp!\big(\boldsymbol{q}^{(h)}_1\cdot \boldsymbol{k}^{(h)}_1\big) }} &amp;
0 &amp; 0 \[2.5em]
\textcolor{blue}{\dfrac{ \exp!\big(\boldsymbol{q}^{(h)}_2\cdot \boldsymbol{k}^{(h)}_1\big) }
      { \exp!\big(\boldsymbol{q}^{(h)}_2\cdot \boldsymbol{k}^{(h)}_1\big)
       +\exp!\big(\boldsymbol{q}^{(h)}_2\cdot \boldsymbol{k}^{(h)}_2\big) }} &amp;
\textcolor{blue}{\dfrac{ \exp!\big(\boldsymbol{q}^{(h)}_2\cdot \boldsymbol{k}^{(h)}_2\big) }
      { \exp!\big(\boldsymbol{q}^{(h)}_2\cdot \boldsymbol{k}^{(h)}_1\big)
       +\exp!\big(\boldsymbol{q}^{(h)}_2\cdot \boldsymbol{k}^{(h)}_2\big) }} &amp;
0 \[2.5em]
\dfrac{ \exp!\big(\boldsymbol{q}^{(h)}_3\cdot \boldsymbol{k}^{(h)}_1\big) }
      { \exp!\big(\boldsymbol{q}^{(h)}_3\cdot \boldsymbol{k}^{(h)}_1\big)
       +\exp!\big(\boldsymbol{q}^{(h)}_3\cdot \boldsymbol{k}^{(h)}_2\big)
       +\exp!\big(\boldsymbol{q}^{(h)}_3\cdot \boldsymbol{k}^{(h)}_3\big) } &amp;
\dfrac{ \exp!\big(\boldsymbol{q}^{(h)}_3\cdot \boldsymbol{k}^{(h)}_2\big) }
      { \exp!\big(\boldsymbol{q}^{(h)}_3\cdot \boldsymbol{k}^{(h)}_1\big)
       +\exp!\big(\boldsymbol{q}^{(h)}_3\cdot \boldsymbol{k}^{(h)}_2\big)
       +\exp!\big(\boldsymbol{q}^{(h)}_3\cdot \boldsymbol{k}^{(h)}_3\big) } &amp;
\dfrac{ \exp!\big(\boldsymbol{q}^{(h)}_3\cdot \boldsymbol{k}^{(h)}_3\big) }
      { \exp!\big(\boldsymbol{q}^{(h)}_3\cdot \boldsymbol{k}^{(h)}_1\big)
       +\exp!\big(\boldsymbol{q}^{(h)}_3\cdot \boldsymbol{k}^{(h)}_2\big)
       +\exp!\big(\boldsymbol{q}^{(h)}_3\cdot \boldsymbol{k}^{(h)}_3\big) }
\end{bmatrix}
$$</p>
<p>Single head attention output
$$
\begin{bmatrix}
\textcolor{blue}{\boldsymbol{o}^{(h)}<em>1} \[1em]
\textcolor{blue}{\boldsymbol{o}^{(h)}_2} \[1em]
\boldsymbol{o}^{(h)}_3
\end{bmatrix}=
\begin{bmatrix}
\textcolor{blue}{A^{(h)}</em>{11} \boldsymbol{v}^{(h)}<em>1} \[1em]
\textcolor{blue}{A^{(h)}</em>{21} \boldsymbol{v}^{(h)}<em>1 +
A^{(h)}</em>{22} \boldsymbol{v}^{(h)}<em>2} \[1em]
A^{(h)}</em>{31} \boldsymbol{v}^{(h)}<em>1 +
A^{(h)}</em>{32} \boldsymbol{v}^{(h)}<em>2 +
A^{(h)}</em>{33} \boldsymbol{v}^{(h)}_3
\end{bmatrix}
$$</p>
<h3 id="multi-head-attention-output_1">Multi head attention output</h3>
<div class="arithmatex">\[
\begin{bmatrix}
\textcolor{blue}{\boldsymbol{o}_1} \\[1em]
\textcolor{blue}{\boldsymbol{o}_2} \\[1em]
\boldsymbol{o}_3
\end{bmatrix}=
\begin{bmatrix}
\textcolor{blue}{\boldsymbol{o}_1^{(1)}} &amp; \textcolor{blue}{\boldsymbol{o}_1^{(2)}} &amp; \ldots &amp;  \textcolor{blue}{\boldsymbol{o}_1^{(H)}}\\[1em]
\textcolor{blue}{\boldsymbol{o}_2^{(1)}} &amp; \textcolor{blue}{\boldsymbol{o}_2^{(2)}} &amp;  \ldots  &amp;\textcolor{blue}{\boldsymbol{o}_2^{(H)}}\\[1em]
\boldsymbol{o}_3^{(1)} &amp; \boldsymbol{o}_3^{(2)} &amp;  \ldots  &amp;\boldsymbol{o}_3^{(H)}\\
\end{bmatrix}
\begin{bmatrix}
\boldsymbol{W}_O^{(1)} \\[1em]
\boldsymbol{W}_O^{(2)} \\[1em]
\vdots \\
\boldsymbol{W}_O^{(H)} \\
\end{bmatrix}=
\begin{bmatrix}
\textcolor{blue}{\sum_{h=1}^H \boldsymbol{o}_1^{(h)}\boldsymbol{W}_O^{(h)}}  \\[1em]
\textcolor{blue}{\sum_{h=1}^H \boldsymbol{o}_2^{(h)}\boldsymbol{W}_O^{(h)}}\\[1em]
\sum_{h=1}^H \boldsymbol{o}_3^{(h)}\boldsymbol{W}_O^{(h)}\\
\end{bmatrix}
\]</div>
<h3 id="residual-connection_1">residual connection</h3>
<div class="arithmatex">\[
\begin{bmatrix}
\textcolor{blue}{\boldsymbol{x}^{\text{MLP}}_1} \\[1em]
\textcolor{blue}{\boldsymbol{x}^{\text{MLP}}_2} \\[1em]
\boldsymbol{x}^{\text{MLP}}_3
\end{bmatrix}=
\begin{bmatrix}
\textcolor{blue}{\boldsymbol{o}_1 + \boldsymbol{x}_1}\\[1em]
\textcolor{blue}{\boldsymbol{o}_2 + \boldsymbol{x}_2} \\[1em]
\boldsymbol{o}_3 + \boldsymbol{x}_3
\end{bmatrix}
\]</div>
<h3 id="mlp_1">MLP</h3>
<div class="arithmatex">\[
\begin{bmatrix}
\textcolor{blue}{\boldsymbol{o}_1^{\text{MLP}}} \\[1em]
\textcolor{blue}{\boldsymbol{o}_2^{\text{MLP}}} \\[1em]
\boldsymbol{o}_3^{\text{MLP}}
\end{bmatrix}=
\begin{bmatrix}
\textcolor{blue}{\text{MLP}\Big(\text{norm}(\boldsymbol{x}^{\text{MLP}}_1) \Big)} \\[1em]
\textcolor{blue}{\text{MLP}\Big(\text{norm}(\boldsymbol{x}^{\text{MLP}}_2) \Big)} \\[1em]
\text{MLP}\Big(\text{norm}(\boldsymbol{x}^{\text{MLP}}_3) \Big) \\
\end{bmatrix}
\]</div>
<h3 id="read-out-layer">read out layer</h3>
<div class="arithmatex">\[
\boldsymbol{x}_4 = \text{read out} \left(\text{norm}(\boldsymbol{o}_3^{\text{MLP}} + \boldsymbol{x}^{\text{MLP}}_3) \right)
\]</div>
<p>All the blue parts are <strong>recomputations</strong> we could have avoided: they do not depend on <span class="arithmatex">\(\boldsymbol{x}_3\)</span> at all, and they will also not change when we later generate <span class="arithmatex">\(\boldsymbol{x}_5\)</span>, <span class="arithmatex">\(\boldsymbol{x}_6\)</span>, and so on.</p>
<p>So a naive caching strategy is to store all those values and reuse them whenever we extend the sequence, instead of recomputing them from scratch each time.</p>
<h1 id="kv-cache">KV cache</h1>
<p>KV cache is exactly this idea, but in a more focused and memory-efficient form.
One thing we haven't considered is this:</p>
<ul>
<li>For the purpose of predicting <span class="arithmatex">\(\boldsymbol{x}_4\)</span>, the only thing we need is the <em>last</em> token’s representation.</li>
</ul>
<p>Let’s look more closely at what we actually need to compute for the last token (token 3).
This will make it clear which quantities have to be recomputed and which ones can safely be reused from a cache.</p>
<p>Self-attention block:</p>
<p>We have
$$
\boldsymbol{o}<em>3^{\text{Attn}} = \sum</em>{h=1}^H \boldsymbol{o}<em>3^{(h)}\boldsymbol{W}_O^{(h)}
$$
where the output of head <span class="arithmatex">\(h\)</span> is
$$
\boldsymbol{o}_3^{(h)} = A^{(h)}</em>{31} \boldsymbol{v}^{(h)}<em>1 +
A^{(h)}</em>{32} \boldsymbol{v}^{(h)}<em>2 +
A^{(h)}</em>{33} \boldsymbol{v}^{(h)}<em>3,
$$
$$
\begin{bmatrix}
A^{(h)}</em>{31}, &amp; A^{(h)}<em>{32}, &amp; A^{(h)}</em>{33}
\end{bmatrix}
=
\text{Softmax}\left( \begin{bmatrix}
\boldsymbol{q}^{(h)}_3 \cdot \boldsymbol{k}^{(h)}_1, &amp; \boldsymbol{q}^{(h)}_3 \cdot \boldsymbol{k}^{(h)}_2, &amp; \boldsymbol{q}^{(h)}_3 \cdot \boldsymbol{k}^{(h)}_3
\end{bmatrix}\right)
$$</p>
<p>So basically we need
$$
\Big{\boldsymbol{q}^{(h)}<em>3, \;\boldsymbol{k}^{(h)}_3,\; \boldsymbol{v}^{(h)}_3, \; \boldsymbol{k}^{(h)}_1,\boldsymbol{k}^{(h)}_2,\;
        \boldsymbol{v}^{(h)}_1,\boldsymbol{v}^{(h)}_2\Big}</em>{h=1}^H
$$</p>
<p>We can further split this into:</p>
<ul>
<li>
<p><strong>What we need to compute</strong> (depends on <span class="arithmatex">\(\boldsymbol{x}_3\)</span>):
  $$
  \Big{\boldsymbol{q}^{(h)}<em>3,\; \boldsymbol{k}^{(h)}_3,\; \boldsymbol{v}^{(h)}_3\Big}</em>{h=1}^H,
  $$
  where
  $$
  \boldsymbol{q}^{(h)}_3 = \widetilde{\boldsymbol{x}}_3 \boldsymbol{W}^{(h)}_Q, \qquad
  \boldsymbol{k}^{(h)}_3 = \widetilde{\boldsymbol{x}}_3 \boldsymbol{W}^{(h)}_K, \qquad
  \boldsymbol{v}^{(h)}_3 = \widetilde{\boldsymbol{x}}_3 \boldsymbol{W}^{(h)}_V.
  $$</p>
</li>
<li>
<p><strong>What we can reuse</strong>:
  $$
  \Big{\boldsymbol{k}^{(h)}<em>1,\boldsymbol{k}^{(h)}_2,\;
        \boldsymbol{v}^{(h)}_1,\boldsymbol{v}^{(h)}_2\Big}</em>{h=1}^H.
  $$</p>
</li>
</ul>
<p>Then, the rest of the forward pass depends only on token 3.</p>
<p>Residual:</p>
<div class="arithmatex">\[
\boldsymbol{x}^{\text{MLP}}_3 = \boldsymbol{o}_3^{\text{Attn}} + \boldsymbol{x}_3.
\]</div>
<p>MLP:</p>
<div class="arithmatex">\[
\boldsymbol{o}^{\text{MLP}}_3 = \text{MLP}\Big(\text{norm}(\boldsymbol{x}^{\text{MLP}}_3) \Big).
\]</div>
<p>Readout:</p>
<div class="arithmatex">\[
\boldsymbol{x}_4 = \text{read out} \left(\text{norm}(\boldsymbol{o}_3^{\text{MLP}} + \boldsymbol{x}^{\text{MLP}}_3) \right)
\]</div>
<p>This is exactly the structure that the KV cache exploits: we recompute only what depends on the newest token, and we reuse everything that belongs to the past.</p>
<p>In general, each decoding step works like this.</p>
<p>For each layer <span class="arithmatex">\(l\)</span> (and each head <span class="arithmatex">\(h\)</span> inside that layer):</p>
<ol>
<li>
<p><strong>Compute projections for the new token</strong><br />
   For the newly appended token at position <span class="arithmatex">\(t\)</span>, we compute <span class="arithmatex">\(\boldsymbol{q}^{(l,h)}_t, \; \boldsymbol{k}^{(l,h)}_t, \; \boldsymbol{v}^{(l,h)}_t\)</span>.</p>
</li>
<li>
<p><strong>Read all previously cached keys and values</strong><br />
   We load <span class="arithmatex">\(\{\boldsymbol{k}^{(l,h)}_1, \dots, \boldsymbol{k}^{(l,h)}_{t-1}\}\)</span> and <span class="arithmatex">\(\{\boldsymbol{v}^{(l,h)}_1, \dots, \boldsymbol{v}^{(l,h)}_{t-1}\}\)</span> from the KV cache.<br />
   Due to causal masking, these past entries never change after they are first computed.</p>
</li>
<li>
<p><strong>Do attention only for the new token</strong><br />
   We compute attention scores using <span class="arithmatex">\(\boldsymbol{q}^{(l,h)}_t\)</span> against all cached keys, apply softmax, and then compute the single-token output <span class="arithmatex">\(\boldsymbol{o}^{(l,h)}_t\)</span>.</p>
</li>
<li>
<p><strong>Append the new key and value to the cache</strong><br />
   After using <span class="arithmatex">\(\boldsymbol{k}^{(l,h)}_t\)</span> and <span class="arithmatex">\(\boldsymbol{v}^{(l,h)}_t\)</span> for attention, we store them so they can be reused in all future steps.</p>
</li>
<li>
<p><strong>Finish the rest of the layer normally</strong><br />
   The attention outputs across heads are combined with the output projection, added to the residual stream, normalized, and passed through the MLP.<br />
   Importantly, <strong>these computations happen only for token <span class="arithmatex">\(t\)</span></strong>, we do not recompute anything for previous tokens.</p>
</li>
</ol>
<p>This repeats for every layer from bottom to top, and the final hidden state of token <span class="arithmatex">\(t\)</span> is used by the readout layer to predict token <span class="arithmatex">\(t+1\)</span>.</p>
<p>That’s it. With the full computation laid out, the motivation for KV cache should now be clear.</p>
          </div>
        </section>
  
        <!-- floating TOC on the right -->
        <aside class="blog-toc" aria-label="Table of contents">
          <div class="blog-toc-inner">
            <div class="blog-toc-title">Table of Contents</div>
            <nav id="toc"></nav>
          </div>
        </aside>
      </div>
  
      <div class="footer">
        © 2025 Rui Li
      </div>
    </div>
  </main>
  
  <!-- Auto-generate TOC from headings -->
  <script>
    document.addEventListener("DOMContentLoaded", () => {
      const content = document.getElementById("post-content");
      const tocContainer = document.getElementById("toc");
      if (!content || !tocContainer) return;
  
      const headings = content.querySelectorAll("h1, h2, h3");
      if (!headings.length) return;
  
      const list = document.createElement("ul");
      list.className = "toc-list";
  
      const slugCounts = {};
  
      function slugify(text) {
        let slug = text.toLowerCase()
          .replace(/[^a-z0-9\s-]/g, "")
          .trim()
          .replace(/\s+/g, "-");
        if (!slug) slug = "section";
        if (slugCounts[slug] != null) {
          slugCounts[slug] += 1;
          slug = slug + "-" + slugCounts[slug];
        } else {
          slugCounts[slug] = 0;
        }
        return slug;
      }
  
      headings.forEach(h => {
        if (!h.id) {
          h.id = slugify(h.textContent || h.innerText || "");
        }
        const li = document.createElement("li");
        li.className = "toc-level-" + h.tagName[1]; // 1,2,3
  
        const a = document.createElement("a");
        a.href = "#" + h.id;
        a.textContent = h.textContent || h.innerText || "";
  
        li.appendChild(a);
        list.appendChild(li);
      });
  
      tocContainer.appendChild(list);
    });
  </script>
  </body>
  </html>
  
